---
title: "Midterm Project"
author: "Ammar Alsadadi"
toc: true
number-sections: true
highlight-style: pygments
format: 
  html: 
    code-fold: true
    html-math-method: katex
    embed-resources: true
    self-contained-math: true	
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
##  docx: Never, unless to accommodate a collaborator
---
# Introduction

## Overview

The NYC 311 Service Requests dataset contains reports related to various city issues, including sewer-related complaints. This analysis focuses on two primary sewer-related complaints for 2024:

1. **Street Flooding (SF)**: Reports of standing water or street flooding.
2. **Catch Basin (CB)**: Reports of clogged or defective catch basins that prevent proper drainage.

These complaints provide insights into both immediate street flooding issues and underlying infrastructural challenges.

## Data Dictionary

| Column Name                        | Description                                              | Data Type            |
|-------------------------------------|----------------------------------------------------------|----------------------|
| **Unique Key**                      | Unique identifier for each service request              | Text                |
| **Created Date**                    | Date and time the service request was created           | Floating Timestamp  |
| **Closed Date**                     | Date and time the service request was closed            | Floating Timestamp  |
| **Agency**                           | City agency responsible for the complaint               | Text                |
| **Agency Name**                      | Full name of the responding agency                      | Text                |
| **Complaint Type**                   | High-level category of the complaint                    | Text                |
| **Descriptor**                       | Specific details about the complaint                    | Text                |
| **Location Type**                    | Type of location information available                  | Text                |
| **Incident Zip**                     | ZIP code where the complaint was filed                  | Text                |
| **Incident Address**                 | Address where the complaint was reported                | Text                |
| **Street Name**                      | Street name of incident location                        | Text                |
| **Cross Street 1**                   | First cross street based on geo-validation              | Text                |
| **Cross Street 2**                   | Second cross street based on geo-validation             | Text                |
| **Intersection Street 1**            | First intersecting street based on geo-validation       | Text                |
| **Intersection Street 2**            | Second intersecting street based on geo-validation      | Text                |
| **Address Type**                     | Type of incident location information                   | Text                |
| **City**                              | City of the incident location                           | Text                |
| **Landmark**                          | Name of landmark if applicable                         | Text                |
| **Facility Type**                     | Type of city facility associated with SR               | Text                |
| **Status**                            | Current status of the complaint                         | Text                |
| **Due Date**                          | Expected update date by responding agency              | Floating Timestamp  |
| **Resolution Description**            | Details of the resolution or current status             | Text                |
| **Resolution Action Updated Date**    | Last update of resolution actions                      | Floating Timestamp  |
| **Community Board**                   | Community board information from geo-validation        | Text                |
| **BBL**                               | Borough Block and Lot identifier                       | Text                |
| **Borough**                           | Borough of the incident                                | Text                |
| **X Coordinate (State Plane)**        | Geo-validated X coordinate of the incident location    | Number              |
| **Y Coordinate (State Plane)**        | Geo-validated Y coordinate of the incident location    | Number              |
| **Open Data Channel Type**            | Submission method (Phone, Online, Mobile, etc.)       | Text                |
| **Park Facility Name**                | Name of park facility if applicable                   | Text                |
| **Park Borough**                      | Borough of the park facility                          | Text                |
| **Vehicle Type**                      | Type of TLC vehicle if applicable                     | Text                |
| **Taxi Company Borough**              | Borough of the taxi company if applicable             | Text                |
| **Taxi Pick Up Location**             | Taxi pick-up location if applicable                   | Text                |
| **Bridge Highway Name**               | Name of bridge/highway if applicable                  | Text                |
| **Bridge Highway Direction**          | Direction of bridge/highway issue                     | Text                |
| **Road Ramp**                          | Specifies if the issue occurred on a road or ramp     | Text                |
| **Bridge Highway Segment**            | Segment of the bridge/highway where issue occurred    | Text                |
| **Latitude**                          | Latitude coordinate of the incident location          | Number              |
| **Longitude**                         | Longitude coordinate of the incident location         | Number              |
| **Location**                          | Combination of latitude and longitude                | Location            |

## Exploring Data

```{python}
import pandas as pd

# Load the dataset
file_path = "data/nycflood2024.csv"
df = pd.read_csv(file_path)

# Display basic info about the dataset
df_info = df.info()

# Show first few rows
df_head = df.head(3)

df_info, df_head

```

The dataset contains Street Flooding (SF) and Catch Basin (CB) complaints from 
2024, extracted from 9,483 service requests. Key columns relevant to the 
analysis include that:  
 
- **Descriptor**: Specifies whether the complaint is about Street Flooding or 
Catch Basin Clogged/Flooding.  
- **Created Date**: Indicates when the complaint was reported.  
- **Borough**: Provides the location of the complaint within NYC.
- **Incident Zip**: Provides the ZIP code of the complaint.   
- **Latitude/Longitude**: Contains geographic coordinates for spatial analysis 
and mapping.  


# Answering The Project Questions

## Part A (Data Cleaning)

### Part i
Import the data, rename the columns with our preferred styles.

```{python}
import pandas as pd

# Load the dataset
file_path = "data/nycflood2024.csv"

df = pd.read_csv(file_path)
# Convert column names to lowercase and replace spaces with underscores
df.columns = df.columns.str.lower().str.replace(" ", "_")

# Display cleaned column names
df.columns
```

The column names appear to have been renamed and cleaned properly.
### Part ii
Summarize the missing information. Are there variables that are close to 
completely missing?

```{python}
# Check for missing values
missing_values = df.isnull().sum()

# Calculate the percentage of missing values per column
missing_summary = (df.isnull().sum() / len(df)) * 100

# Identify columns that are completely missing 
highly_missing_columns = missing_summary[missing_summary == 100]

# Display summary of missing data
print(missing_values,missing_summary, highly_missing_columns)
```


Several variables are completely missing (100%), making them irrelevant
for analysis. These include:

- **location_type**
- **landmark**
- **facility_type**
- **due_date**
- **vehicle_type**
- **taxi_company_borough**
- **taxi_pick_up_location**
- **bridge_highway_name**
- **bridge_highway_direction**
- **road_ramp**
- **bridge_highway_segment**

Other Key Variables with Significant Missing Data:

- **incident_address, street_name, cross_street_1, cross_street_2** 
  (23.6% missing) This could impact location-based analysis.
- **intersection_street_1, intersection_street_2** 
  (76.4% missing) These are mostly empty.
- **bbl** (27.8% missing) Used for property identification.
- **closed_date** (2.2% missing) Some complaints are still open.
- **latitude, longitude** (1% missing) A small fraction lacks 
  geographic data.

### Part iii
The following columns appear redundant:  

- **Agency Name** (redundant with "Agency")  
- **Park Borough** (redundant with "Borough")  
- **Location** (duplicates Latitude and Longitude)  
- **State Plane Coordinates (X, Y)** (redundant with Latitude/Longitude) 

```{python}
import pyarrow.feather as feather
import os
# Store as Arrow (Feather) format for efficiency
arrow_file_path = "data/nycflood2024.feather"
feather.write_feather(df, arrow_file_path)

# Compare file sizes
csv_size = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB
arrow_size = os.path.getsize(arrow_file_path) / (1024 * 1024)  # Convert to MB

csv_size, arrow_size, (1 - (arrow_size / csv_size)) * 100 

```

The dataset has been converted to Feather format and is stored in the **data**  
directory. The results are:  

- **Original CSV size:** 5.82 MB  
- **Arrow (Feather) size:** 1.85 MB  
- **Efficiency gain:** 68.2% reduction in storage size  

The Feather format is significantly more storage-efficient than CSV, reducing  
the file size by nearly 70%. This results in:  

- Faster load times  
- Less disk space usage  
- More efficient data processing  

The reduction is mainly due to better compression and efficient data types.  
CSV stores data as raw text, while Arrow uses a binary columnar format,  
resulting in improved compression. Additionally, Arrow optimizes how numerical  
and categorical data are stored, further enhancing storage efficiency.  


### Part iv
Are there invalid NYC zipcode or borough? Can some of the missing values be 
filled? Fill them if yes.
```{python}
# Create a dictionary mapping ZIP codes to boroughs
valid_zip_to_borough = {
    10001: "Manhattan", 10002: "Manhattan", 10003: "Manhattan", 10004: "Manhattan",
    10005: "Manhattan", 10006: "Manhattan", 10007: "Manhattan", 10009: "Manhattan",
    10010: "Manhattan", 10011: "Manhattan", 10012: "Manhattan", 10013: "Manhattan",
    10014: "Manhattan", 10015: "Manhattan", 10016: "Manhattan", 10017: "Manhattan",
    10018: "Manhattan", 10019: "Manhattan", 10020: "Manhattan", 10021: "Manhattan",
    10022: "Manhattan", 10023: "Manhattan", 10024: "Manhattan", 10025: "Manhattan",
    10026: "Manhattan", 10027: "Manhattan", 10028: "Manhattan", 10029: "Manhattan",
    10030: "Manhattan", 10031: "Manhattan", 10032: "Manhattan", 10033: "Manhattan",
    10034: "Manhattan", 10035: "Manhattan", 10036: "Manhattan", 10037: "Manhattan",
    10038: "Manhattan", 10039: "Manhattan", 10040: "Manhattan", 10041: "Manhattan",
    10044: "Manhattan", 10045: "Manhattan", 10048: "Manhattan", 10055: "Manhattan",
    10060: "Manhattan", 10069: "Manhattan", 10090: "Manhattan", 10095: "Manhattan",
    10098: "Manhattan", 10099: "Manhattan", 10103: "Manhattan", 10104: "Manhattan",
    10105: "Manhattan", 10106: "Manhattan", 10107: "Manhattan", 10110: "Manhattan",
    10111: "Manhattan", 10112: "Manhattan", 10115: "Manhattan", 10118: "Manhattan",
    10119: "Manhattan", 10120: "Manhattan", 10121: "Manhattan", 10122: "Manhattan",
    10123: "Manhattan", 10128: "Manhattan", 10151: "Manhattan", 10152: "Manhattan",
    10153: "Manhattan", 10154: "Manhattan", 10155: "Manhattan", 10158: "Manhattan",
    10161: "Manhattan", 10162: "Manhattan", 10165: "Manhattan", 10166: "Manhattan",
    10167: "Manhattan", 10168: "Manhattan", 10169: "Manhattan", 10170: "Manhattan",
    10171: "Manhattan", 10172: "Manhattan", 10173: "Manhattan", 10174: "Manhattan",
    10175: "Manhattan", 10176: "Manhattan", 10177: "Manhattan", 10178: "Manhattan",
    10199: "Manhattan", 10270: "Manhattan", 10271: "Manhattan", 10278: "Manhattan",
    10279: "Manhattan", 10280: "Manhattan", 10281: "Manhattan", 10282: "Manhattan",
    10301: "Staten Island", 10302: "Staten Island", 10303: "Staten Island",
    10304: "Staten Island", 10305: "Staten Island", 10306: "Staten Island",
    10307: "Staten Island", 10308: "Staten Island", 10309: "Staten Island",
    10310: "Staten Island", 10311: "Staten Island", 10312: "Staten Island",
    10314: "Staten Island", 10451: "Bronx", 10452: "Bronx", 10453: "Bronx",
    10454: "Bronx", 10455: "Bronx", 10456: "Bronx", 10457: "Bronx", 10458: "Bronx",
    10459: "Bronx", 10460: "Bronx", 10461: "Bronx", 10462: "Bronx", 10463: "Bronx",
    10464: "Bronx", 10465: "Bronx", 10466: "Bronx", 10467: "Bronx", 10468: "Bronx",
    10469: "Bronx", 10470: "Bronx", 10471: "Bronx", 10472: "Bronx", 10473: "Bronx",
    10474: "Bronx", 10475: "Bronx", 11004: "Queens", 11101: "Queens",
    11102: "Queens", 11103: "Queens", 11104: "Queens", 11105: "Queens",
    11106: "Queens", 11109: "Queens", 11201: "Brooklyn", 11203: "Brooklyn",
    11204: "Brooklyn", 11205: "Brooklyn", 11206: "Brooklyn", 11207: "Brooklyn",
    11208: "Brooklyn", 11209: "Brooklyn", 11210: "Brooklyn", 11211: "Brooklyn",
    11212: "Brooklyn", 11213: "Brooklyn", 11214: "Brooklyn", 11215: "Brooklyn",
    11216: "Brooklyn", 11217: "Brooklyn", 11218: "Brooklyn", 11219: "Brooklyn",
    11220: "Brooklyn", 11221: "Brooklyn", 11222: "Brooklyn", 11223: "Brooklyn",
    11224: "Brooklyn", 11225: "Brooklyn", 11226: "Brooklyn", 11228: "Brooklyn",
    11229: "Brooklyn", 11230: "Brooklyn", 11231: "Brooklyn", 11232: "Brooklyn",
    11233: "Brooklyn", 11234: "Brooklyn", 11235: "Brooklyn", 11236: "Brooklyn",
    11237: "Brooklyn", 11238: "Brooklyn", 11239: "Brooklyn", 11249: "Brooklyn",
    11351: "Queens", 11354: "Queens", 11355: "Queens", 11356: "Queens",
    11357: "Queens", 11358: "Queens", 11359: "Queens", 11360: "Queens",
    11361: "Queens", 11362: "Queens", 11363: "Queens", 11364: "Queens",
    11365: "Queens", 11366: "Queens", 11367: "Queens", 11368: "Queens",
    11369: "Queens", 11370: "Queens", 11371: "Queens", 11372: "Queens",
    11373: "Queens", 11374: "Queens", 11375: "Queens", 11377: "Queens",
    11378: "Queens", 11379: "Queens", 11385: "Queens", 11411: "Queens",
    11412: "Queens", 11413: "Queens", 11414: "Queens", 11415: "Queens",
    11416: "Queens", 11417: "Queens", 11418: "Queens", 11419: "Queens",
    11420: "Queens", 11421: "Queens", 11422: "Queens", 11423: "Queens",
    11426: "Queens", 11427: "Queens", 11428: "Queens", 11429: "Queens",
    11430: "Queens", 11432: "Queens", 11433: "Queens", 11434: "Queens",
    11435: "Queens", 11436: "Queens", 11691: "Queens", 11692: "Queens",
    11693: "Queens", 11694: "Queens", 11697: "Queens"
}

# Check invalid ZIP codes
df["incident_zip"] = df["incident_zip"].astype(float).astype("Int64")
invalid_zipcodes = df[~df["incident_zip"].isin(valid_zip_to_borough.keys())]["incident_zip"].unique()

# Valid boroughs
valid_boroughs = {"MANHATTAN", "BRONX", "BROOKLYN", "QUEENS", "STATEN ISLAND"}

# Identify invalid boroughs
invalid_boroughs = df[~df["borough"].str.upper().isin(valid_boroughs)]["borough"].unique()

invalid_zipcodes,invalid_boroughs
```


The list of valid NYC ZIP codes was sourced from NYC by Natives to check for  
validity.  

The dataset includes five ZIP codes that are not present in the provided list  
of valid NYC ZIP codes:  

- **NaN (Missing ZIP codes):** Some records lack a ZIP code.  
- **11040, 11001:** These belong to Nassau County, Long Island, NY, and are  
  not part of NYC. However, since Nassau County borders Queens, some  
  addresses may still be functionally tied to NYC, so it may be safer to  
  retain them in the dataset.  
- **10065, 10075:** These are valid NYC ZIP codes (Upper East Side,  
  Manhattan) but were excluded from the provided list. However, they appear  
  on an alternative source.  

Additionally, some records have "Unspecified" as the borough instead of a  
valid NYC borough.  

All of these ZIP codes appear valid in context, so none will be removed.  
Missing ZIP codes will be filled using longitude and latitude data, and 
"Unspecified" boroughs will be filled using the zip code info we collected.

Lets first look at the rows with "Unspecified" Borough:

```{python}
# Identify Unique IDs of rows that  had "Unspecified" boroughs
unspecified_borough_ids_before = df[
    df["borough"].str.upper() == "UNSPECIFIED"
]["unique_key"].unique()

unspecified_borough_ids_before
```

There is only two rows with "Unspecified" Boroughs, now lets fill them.
```{python}

#Fill missing boroughs using the ZIP to borough mapping
df["borough"] = df.apply(
    lambda row: valid_zip_to_borough.get(row["incident_zip"], row["borough"]),
    axis=1
)

updated_boroughs = df[df["unique_key"].isin(unspecified_borough_ids_before)][["unique_key", "borough"]]

# Display results
updated_boroughs
```

Now that all boroughs have values, we will convert the entire column to 
lowercase to improve consistency and standardization, making it easier to
reference.

```{python}
df["borough"] = df["borough"].astype(str).str.lower()
df["borough"].head()
```

Good. Now, let's move on to ZIP codes and determine the number of rows with 
missing ZIP codes.

```{python}
# Identify unique IDs of rows with missing values in the "incident_zip" column
missing_zipcode_ids = df[df["incident_zip"].isna()]["unique_key"].unique()

missing_zipcode_ids
```

There seem to be four rows with missing zipcodes, lets see if they have longitude and latitude data to fill them:

```{python}
zip_lat_long = df[df["unique_key"].isin(missing_zipcode_ids)][["unique_key", "longitude","latitude"]]

zip_lat_long
```

They all have to be missing the longitude and latitude data. At first, I  
considered using geocoders to determine ZIP codes based on street addresses.  
However, I realized that a single street address can correspond to multiple  
ZIP codes, making it unreliable for accurate data filling.  

### Part v
Are there date errors? Examples are earlier closed_date than created_date; closed_date and created_date matching to the second; dates exactly at midnight or noon to the second.

```{python}
# Convert date columns to datetime format for analysis
import pandas as pd

df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')
df['closed_date'] = pd.to_datetime(df['closed_date'], errors='coerce')

# Identify records where closed_date is earlier than created_date
closed_earlier_than_created = df[df['closed_date'] < df['created_date']][['unique_key', 'created_date', 'closed_date']]

# Identify records where closed_date and created_date are exactly the same
exact_match_dates = df[df['closed_date'] == df['created_date']][['unique_key', 'created_date', 'closed_date']]

# Identify records where created_date is exactly at midnight or noon
created_at_midnight_or_noon = df[df['created_date'].dt.time.isin([pd.Timestamp('00:00:00').time(), pd.Timestamp('12:00:00').time()])][['unique_key', 'created_date', 'closed_date']]

# Identify records where closed_date is exactly at midnight or noon
closed_at_midnight_or_noon = df[df['closed_date'].dt.time.isin([pd.Timestamp('00:00:00').time(), pd.Timestamp('12:00:00').time()])][['unique_key', 'created_date', 'closed_date']]

# Print results
print("Closed Earlier Than Created:")
print(closed_earlier_than_created)
print("\nExact Match Dates:")
print(exact_match_dates)
print("\nMidnight or Noon Created Dates:")
print(created_at_midnight_or_noon)
print("\nMidnight or Noon Closed Dates:")
print(closed_at_midnight_or_noon)

```

Findings on Date Errors:  

- **1 instance** where the closed_date occurs before the created_date, which  
  is likely an error in data entry or processing.  

- **160 instances** where both created_date and closed_date match exactly to  
  the second. This may suggest potential data entry anomalies.  

- **0 instances** where the created_date is exactly at 00:00:00 or 12:00:00.  
  This suggests that request creation times are generally varied and not  
  rounded to standard time markers.  

- **202 instances** where the closed_date is at 00:00:00 or 12:00:00. This  
  pattern may indicate systematic closing times, automated processing, or  
  data standardization.   

These findings suggest a mix of normal system behavior and possible data  
inconsistencies. The single case of an earlier closed_date is a clear anomaly,  
while the frequent exact-match timestamps and standard time closures could  
warrant further investigation into data entry or automated logging processes.  

### Part vi
Summarize your suggestions to the data curator in several bullet points.


Suggestions for Data Curation  

- **Fix Date Anomalies:**  
  - Investigate and correct the 1 case where closed_date is earlier than  
    created_date.  
  - Review 160 cases where created_date and closed_date are identical to  
    ensure accurate resolution timestamps.  
  - Examine why 202 cases have closed_date exactly at midnight or noon, this  
    might indicate system generated timestamps rather than actual resolution  
    times.  

- **Address Missing and Inconsistent Data:**  
  - Missing ZIP codes that also lack latitude and longitude should be reviewed  
    for potential recovery from other available location data.  
  - Some rows have unspecified boroughs, which might impact geographic  
    analysis, consider validating or imputing these values.  

- **Remove Redundant Columns to Improve Data Efficiency:**  
  - Agency Name (redundant with Agency)  
  - Park Borough (redundant with Borough)  
  - Location (duplicates Latitude and Longitude)  
  - State Plane Coordinates (X, Y) (redundant with Latitude/Longitude)  

- **Standardize Data Entry and Logging Processes:**  
  - Ensure a consistent methodology for logging timestamps to prevent  
    artificial patterns.  
  - Improve the accuracy of missing or vague location details by  
    cross referencing available datasets.  

- **Handle Completely Empty and Highly Missing Columns:**  
  - The following columns are completely empty and provide no useful data:  
    - Location Type  
    - Landmark  
    - Facility Type  
    - Due Date  
    - Vehicle Type  
    - Taxi Company Borough  
    - Taxi Pick Up Location  
    - Bridge Highway Name  
    - Bridge Highway Direction  
    - Road Ramp  
    - Bridge Highway Segment  


## Part B (Exploratory analysis)

### Part i
Visualize the locations of complaints on a NYC map, with different symbols for different descriptors.
```{python}
import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx

# Filter out rows with missing latitude, longitude, or descriptor
df_filtered = df.dropna(subset=['latitude', 'longitude', 'descriptor'])

# Convert to a GeoDataFrame
gdf = gpd.GeoDataFrame(df_filtered, geometry=gpd.points_from_xy(df_filtered.longitude, df_filtered.latitude), crs="EPSG:4326")

gdf = gdf.to_crs(epsg=3857)

# Define descriptor categories, marker styles, and sizes
descriptor_markers = {
    'Street Flooding (SJ)': 'o',  # Circles
    'Catch Basin Clogged/Flooding (Use Comments) (SC)': 's'  # Squares
}
descriptor_colors = {
    'Street Flooding (SJ)': 'red',
    'Catch Basin Clogged/Flooding (Use Comments) (SC)': 'blue'
}

# Create figure
fig, ax = plt.subplots(figsize=(10, 10))

# Plot each descriptor type separately
for descriptor, marker in descriptor_markers.items():
    subset = gdf[gdf['descriptor'] == descriptor]
    ax.scatter(
        subset.geometry.x, subset.geometry.y, 
        s=20,  # Standard size for markers
        c=descriptor_colors[descriptor], 
        label=descriptor, 
        marker=marker, alpha=0.6
    )

# Add basemap
ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)

# Add legend and title
ax.legend(title="Complaint Descriptors", loc='upper right')
ax.set_title("NYC Flood Complaints on a Static Map")

# Remove x and y axes
ax.set_xticks([])
ax.set_yticks([])
ax.set_frame_on(False)

# Show the plot
plt.show()

```

#### Reflection on Using a Static Map for NYC Flood Complaints  

I initially considered using interactive maps (e.g., Folium) to visualize NYC flood  
complaints but ultimately opted for a static map using GeoPandas and Matplotlib.  
Below is my reasoning and the approaches I tested.

- **Clear & Visible at All Zoom Levels**  
  - Interactive maps hid some complaints until zoomed in.  
  - Folium’s MarkerCluster grouped markers, making it hard to distinguish 
    overlapping points.
  - A static map ensures all complaints remain visible.

- **Customization & Flexibility**  
  - Used different shapes, o for circles, s for squares) to differentiate  
    complaint types.  
  - Adjusted transparency (alpha=0.6) to keep overlapping points visible.

- **Other Ideas I Explored**  
  - Interactive Map (Folium) with Marker Clustering
     - Effective when zoomed in, but one descriptor dominated at lower zoom levels.  
  - Grouping Complaints by Location
    - Reduced overlap but lost individual report granularity.

- **Final Thoughts**  
  A static map was the best choice because it ensured visibility of all complaints,  
  provided clear differentiation, and allowed for effective presentation.

#### Further Visualization  

While analyzing the static map, I noticed that Catch Basin complaints appear
more frequently than Street Flooding complaints, which led me to investigate 
the distribution further. To quantify this difference, I decided to create 
a pie chart to visualize the percentage share of each complaint type and a bar 
chart to compare their counts.

```{python}
import matplotlib.pyplot as plt

# Replace long descriptor with "Catch Basin"
df_filtered['descriptor'] = df_filtered['descriptor'].replace("Catch Basin Clogged/Flooding (Use Comments) (SC)", "Catch Basin")

# Count occurrences of each descriptor
descriptor_counts = df_filtered['descriptor'].value_counts()

# Define colors
colors = ['blue', 'red']

# Plot pie chart
descriptor_counts.plot.pie(autopct='%1.1f%%', colors=colors, startangle=90, figsize=(8, 6), title="Percentage Distribution of Flood Complaints")
plt.ylabel("")  # Hide y-axis label for cleaner look
plt.show()

# Plot bar chart
descriptor_counts.plot.bar(color=colors, alpha=0.7, figsize=(8, 6), title="Count of NYC Flood Complaints by Type")
plt.ylabel("Count of Complaints")
plt.show()

```
My intuition was correct, approximately two-thirds of the complaints are related
to catch basins, while one-third are for street flooding.


### Part ii 
Create a variable response_time, which is the duration from created_date to closed_date.

```{python}
# Convert created_date and closed_date to datetime format
df['created_date'] = pd.to_datetime(df['created_date'], errors='coerce')
df['closed_date'] = pd.to_datetime(df['closed_date'], errors='coerce')

# Calculate response_time as the duration from created_date to closed_date
df['response_time'] = (df['closed_date'] - df['created_date']).dt.total_seconds() / 3600  # Convert to hours

# Check if variable was created

df["response_time"].head()
```

The varaible was created, I calculated response_time by converting both
created_date and closed_date to datetime format, then subtracting created_date 
from closed_date. The result was converted to hours using
.dt.total_seconds() / 3600, allowing for easier analysis of how long it took to
resolve each complaint.

### Part iii
Visualize the comparison of response time by complaint descriptor and borough. 
The original may not be the best given the long tail or outlers.

```{python}
import seaborn as sns
import numpy as np

# Remove negative response times (likely data errors)
df_filtered = df[df['response_time'] >= 0].copy()

# Remove extreme outliers using the IQR method
Q1 = df_filtered['response_time'].quantile(0.25)
Q3 = df_filtered['response_time'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

df_filtered = df_filtered[(df_filtered['response_time'] >= lower_bound) & (df_filtered['response_time'] <= upper_bound)]

# Create a boxplot using seaborn to compare response times by complaint type and borough
plt.figure(figsize=(12, 6))
sns.boxplot(
    data=df_filtered, x='borough', y='response_time', hue='descriptor', 
    showfliers=False, palette={"Street Flooding (SJ)": "red", "Catch Basin Clogged/Flooding (Use Comments) (SC)": "blue"}
)
plt.title("Response Time Comparison by Complaint Type and Borough (Without Outliers)")
plt.ylabel("Response Time (Hours)")
plt.xlabel("Borough")
plt.legend(title="Complaint Type")
plt.xticks(rotation=45)
plt.show()
```

I removed negative response times since they are likely data entry errors and 
do not make logical sense. To handle outliers, I used the Interquartile Range
(IQR) method, which removes extreme values beyond 1.5 times the IQR. This helps
focus on typical response times without skewing the visualization. For 
visualization, I chose a Seaborn boxplot to effectively compare median response
times and variability across boroughs while highlighting differences between
Street Flooding (SF) and Catch Basin Clogged (CB) complaints

#### Analysis of Response Time Comparison by Complaint Type and Borough

- **Catch Basin Complaints Have Longer Response Times**  
  - The blue boxplots (representing Catch Basin complaints) generally show 
    higher median response times and wider interquartile ranges (IQRs) compared
    to street flooding complaints.  
  - Particularly in Staten Island, the Bronx, and Queens, response times for  
    catch basin complaints show significant variation, with upper whiskers  
    extending well beyond 100 hours in some cases.

- **Street Flooding Complaints Are Resolved Faster**  
  - The red boxplots (representing Street Flooding complaints) consistently  
    show lower median response times across all boroughs.  
  - In Manhattan, response times for street flooding are especially low, with  
    most complaints resolved in under 10 hours, suggesting a quicker resolution  
    process compared to other boroughs.

- **High Variability in Staten Island and the Bronx**  
  - Staten Island shows the widest range in response times for catch basin  
    complaints, with some cases taking well over 150 hours to resolve.  
  - The Bronx also exhibits a large spread in response times, particularly for  
    catch basin complaints, though street flooding complaints remain relatively  
    consistent.

- **Brooklyn and Manhattan Have the Fastest Response Times Overall**  
  - Brooklyn’s response times for both complaint types are relatively low and  
    consistent, with fewer extreme outliers.  
  - Manhattan has the shortest response times for both street flooding and  
    catch basin complaints, indicating efficient response handling in this  
    borough.

### Part iv
Is there significant difference in response time between SF and CB complaints? 
Across different boroughs? Does the difference between SF and CB depend on 
borough? State your hypothesis, justify your test, and summarize your results 
in plain English.

#### Is there significant difference in response time between SF and CB complaints? 

First we will test "Is there significant difference in response time between
SF and CB complaints?":

- **Hypothesis**  
  - **Null Hypothesis (H₀):** There is no significant difference in response  
    times between Street Flooding (SF) and Catch Basin (CB) complaints.  
  - **Alternative Hypothesis (H₁):** There is a significant difference in  
    response times between SF and CB complaints.  

Welch’s T-test is ideal for comparing two groups with potentially unequal
variances and sample sizes, making it well-suited for analyzing response
times across different complaint types. Unlike a standard T-test, it does not
assume equal variances, providing a more reliable estimate in cases where 
distribution and variability differ.


```{python}
import scipy.stats as stats

df_filtered = df.dropna(subset=['response_time', 'descriptor', 'borough'])
df_filtered = df_filtered[df_filtered['response_time'] >= 0]

# Separate response times for SF and CB
sf_times = df_filtered[df_filtered['descriptor'] == 'Street Flooding (SJ)']['response_time']
cb_times = df_filtered[df_filtered['descriptor'] == 'Catch Basin']['response_time']

# Perform an independent t-test
t_stat, p_value = stats.ttest_ind(sf_times, cb_times, equal_var=False)

print(t_stat,p_value)
```

The p-value is nearly zero, which is far below the 0.05.This means we reject 
the null hypothesis and conclude that response times for SF and CB 
complaints are significantly different.

#### Is there significant difference in response time borough? 

Now lets test "Is there significant difference in response time between SF and CB complaints?":
- **Hypothesis**  

    - **Null Hypothesis (H₀):**  
    There is no significant difference in response times across different 
    boroughs.

    - **Alternative Hypothesis (H₁):**  
    There is a significant difference in response times across different 
    boroughs.

One-Way ANOVA is used to compare response times across multiple boroughs to 
determine if significant differences exist. It analyzes variations in mean 
response times across more than two groups.

```{python}
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Define the ANOVA model
anova_model = ols('response_time ~ C(borough)', data=df_filtered).fit()

# Perform ANOVA
anova_results = sm.stats.anova_lm(anova_model, typ=2)

# Display ANOVA results
anova_results
```

Since this is far below 0.05, we reject the null hypothesis and conclude that 
response times vary significantly across boroughs.

#### Does the difference between SF and CB depend on borough?
Now lets test "Does the difference between SF and CB depend on borough?":

Two-Way ANOVA examines the effects of both borough and complaint type on response times, as well as their interaction. This test helps determine whether borough influences the difference in response times between SF and CB complaints.

**Null Hypothesis (H₀):**  
    - The difference between SF and CB does not depend on the borough.

**Alternative Hypothesis (H₁):**  
    - The difference in response times depends on the borough (some boroughs may  
    respond faster to one type of complaint than another).

```{python}
# Analyze response time across different boroughs
borough_stats = df_filtered.groupby(['borough', 'descriptor'])['response_time'].mean().unstack()

# Test interaction effect: Does the difference between SF and CB depend on borough
# Perform a two-way ANOVA test
anova_model = ols('response_time ~ C(descriptor) * C(borough)', data=df_filtered).fit()
anova_results = sm.stats.anova_lm(anova_model, typ=2)
anova_results
```
Response times differ significantly between SF and CB complaints. The large  
F-statistic (128.20) and a small p-value lower than 0.05 confirm that SF and CB  
complaints are resolved at significantly different speeds.

Response times also vary across boroughs. The high F-statistic (46.06) and a  
small p-value lower than 0.05 indicate that boroughs differ significantly in  
complaint resolution speed, with some responding much faster or slower than  
others.

Additionally, borough impacts how SF and CB response times compare. The small  
interaction effect and a p-value lower than 0.05 confirm that the gap between  
SF and CB response times depends on the borough, meaning different areas  
prioritize complaint types differently.

### Part v
Create a binary variable over3d to indicate that a service request took three days or longer to close.

```{python}
# Create the binary variable 'over3d' (1 if response_time is 72 hours or more, else 0)
df['over3d'] = (df['response_time'] >= 72).astype(int)

```

I have added the column, but now lets check if it works

```{python}
# Check if all rows where over3d = 1 have response_time >= 72
check_over3d = df[df['over3d'] == 1]['response_time'].min()  # Should be at least 72

# Check if all rows where over3d = 0 have response_time < 72
check_under3d = df[df['over3d'] == 0]['response_time'].max()  # Should be less than 72

print(check_under3d)
print(check_over3d)

```
It works,as the minimum response time for category 1 exceeds 72 hours, while the
maximum response time for category 0 remains below 72 hours.

### Patt vi
Does over3d depend on the complaint descriptor, borough, or weekday
(vs weekend/holiday)? State your hypotheses, justify your test, and 
summarize your results.


I will use a logistic regression because the outcome variable, over3d, is binary 
(1 = complaint took 3+ days, 0 = resolved in less than 3 days). This model is 
ideal for estimating the probability of a complaint exceeding three days based 
on categorical predictors like complaint type, borough, and whether it was filed
on a weekend or holiday. Unlike linear regression, logistic regression ensures 
predicted probabilities remain between 0 and 1, making it the best choice for 
this binary classification problem.

Hypotheses:

**Effect of Complaint Type**

**Null Hypothesis (H₀):**  
   - The complaint type does not affect whether it takes 3+ days to resolve.

**Alternative Hypothesis (H₁):**  
   - The complaint type does affect whether it takes 3+ days to resolve.

**Effect of Borough**

**Null Hypothesis (H₀):**  
   - The borough does not influence the likelihood of taking 3+ days.

**Alternative Hypothesis (H₁):**  
   - The borough does have an effect.

**Effect of Weekend/Holiday vs. Weekday Reporting**

**Null Hypothesis (H₀):**  
   - Complaints reported on weekends/holidays have the same probability of  
     taking 3+ days as those reported on weekdays.

**Alternative Hypothesis (H₁):**  
   - Complaints reported on weekends/holidays are more/less likely to take 3+  
     days.

```{python}
from statsmodels.formula.api import logit

# Create a binary variable for weekday vs. weekend/holiday
df['weekday'] = df['created_date'].dt.weekday  # 0=Monday, 6=Sunday
df['weekend_holiday'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)  # 1 for Saturday/Sunday

# Logistic Regression Model (over3d as the dependent variable)
logit_model = logit("over3d ~ C(descriptor) + C(borough) + C(weekend_holiday)", data=df).fit()

# Get summary results
logit_results = logit_model.summary()

# Display results
logit_results

```

The logistic regression model estimates the likelihood of a complaint remaining  
open for more than three days, with the intercept representing the baseline  
probability for Catch Basin complaints in the Bronx on a weekday.

**Effect of Complaint Type:**  
   - Street Flooding complaints are resolved faster than Catch Basin  
     complaints.  
   - The large negative coefficient (-1.2855, p < 0.001) indicates that SF  
     complaints are significantly less likely to remain open for more than  
     three days, suggesting that Catch Basin issues face more delays.

**Effect of Borough:**  
   - Response times vary by borough.  
   - Brooklyn has the fastest response times, showing the strongest negative  
     effect (p < 0.001).  
   - Manhattan and Queens also see quicker resolutions compared to the Bronx,  
     but the difference is smaller.  
   - Staten Island, however, is not significantly different from the Bronx  
     (p = 0.167).

**Effect of Weekend/Holiday Reporting:**  
   - Complaints filed on weekends or holidays are slightly less likely to remain  
     open for over three days.  
   - The negative coefficient (-0.1754, p = 0.012) suggests that weekend  
     complaints may be processed more quickly, possibly due to dedicated  
     emergency response teams or a lower overall volume of complaints.

Based on the small p-values, all null hypotheses were rejected, confirming that 
complaint type, borough, and whether a complaint was filed on a 
weekend/holiday all significantly impact response time. 

## Part C (Modeling the occurrence of overly long response time)

### Part i 
Create a data set which contains the outcome variable over3d and variables that 
might be useful in predicting it. Consider including time-of-day effects 
(e.g., rush hour vs. late-night), seasonal trends, and neighborhood-level 
demographics. Zip code level information could be useful too, such as the zip 
code area and the ACS 2023 variables.

First lets load the ACS2023 and Areas data.
```{python}
dfacs = pd.read_feather("data/acs2023.feather")
dfareas = pd.read_feather("data/nyc_zip_areas.feather")

print(dfacs.head(2))
print(dfareas.head(2))

```

Lets merge the two datasets with the based on Zip code.

```{python}
# Strip any whitespace from the column names
dfacs.columns = dfacs.columns.str.strip()
dfareas.columns = dfareas.columns.str.strip()
df.columns = df.columns.str.strip()

# Rename the first column of dfareas to 'zip_code'
dfareas.columns.values[0] = "zip_code"

# Rename the last column of dfacs to 'zip_code'
dfacs.columns.values[-1] = "zip_code"

# Rename the nintth column of df to 'zip_code'
df.columns.values[8] = "zip_code"

# Fix ZIP Code Formatting in Dataset
df["zip_code"] = df["zip_code"].astype(str)  # Convert to string
df["zip_code"] = df["zip_code"].str.split('.').str[0]  # Remove decimal point if present
# Merge the dataframes
df = df.merge(dfacs, on="zip_code", how="left")
df = df.merge(dfareas, on="zip_code", how="left")


# Convert column names to lowercase and replace spaces with underscores
df.columns = df.columns.str.lower().str.replace(" ", "_")
print(df.head(1))

```

Now lets create some variables

```{python}
# Extract time-of-day effects
df['hour_of_day'] = df['created_date'].dt.hour  # Extract hour from created date
df['rush_hour'] = df['hour_of_day'].apply(lambda x: 1 if 7 <= x <= 10 or 16 <= x <= 19 else 0)  # Define rush hour

# Extract seasonal trends
df['month'] = df['created_date'].dt.month  # Extract month
df['season'] = df['month'].apply(lambda x: 'winter' if x in [12, 1, 2] else 
                                            'spring' if x in [3, 4, 5] else 
                                            'summer' if x in [6, 7, 8] else 
                                            'fall')


# Dummy variable encoding for season
df = pd.get_dummies(df, columns=['season'], drop_first=True)


# Create a binary variable for weekend/holiday
df['weekday'] = df['created_date'].dt.weekday  # 0=Monday, 6=Sunday
df['weekend_holiday'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)  # 1 for Saturday/Sunday

# Calculate Population Density
df['population_density'] = df['total_population'] / df['land_area_sq_miles']
# Calculate Education Index (Higher Education Rate)
df['education_index'] = (df['bachelor’s_degree_holders'] + df['graduate_degree_holders']) / df['total_population']

# Calculate Unemployment Rate
df['unemployment_rate'] = df['unemployed'] / df['labor_force']

# Calculate Housing Affordability Index
df['housing_affordability_index'] = df['median_home_value'] / df['median_household_income']

df_model = df[['over3d', 'borough', 'descriptor', 'weekend_holiday', 'rush_hour', 
               'season_spring', 'season_summer', 'season_winter',
               'population_density', 'education_index', 
               'unemployment_rate', 'housing_affordability_index']]

print(df_model.head(2))
```

I created these variables to capture demographic and economic factors
that may influence whether a service request takes three or more days (over3d) 
to close. Time based features like rush hour, weekends, and seasons help analyze
response time variations based on when a complaint is filed. Demographic and 
economic indicators such as population density, education levels, unemployment, 
and housing affordability provide insight into how neighborhood characteristics 
impact service efficiency.

### Part ii
Randomly select 20% of the complaints as testing data with seeds 1234. Build a 
logistic model to predict over3d for the complaints with the training data. 
If you have tuning parameters, justify how they were selected.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Drop rows with NaN values in the dataset before training the model
df_model = df_model.dropna()

# Define the features and target variable
X = df_model.drop(columns=['over3d'])  # Features
y = df_model['over3d']  # Target variable

# Convert categorical variables into dummy variables
X = pd.get_dummies(X, drop_first=True)

# Split the data into training (80%) and testing (20%) with a fixed random seed
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)

# Standardize numerical features for better model performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build a logistic regression model
logit_model = LogisticRegression(max_iter=500, solver='lbfgs', random_state=1234)
logit_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = logit_model.predict(X_test_scaled)

```

**Tuning Parameters**

- **max_iter=500:**  
  - Increased iterations to ensure convergence, as logistic regression may  
    struggle with large datasets.  

- **solver='lbfgs':**  
  - Default and efficient solver.  

- **StandardScaler():**  
  - Standardized numerical features to improve model stability and prevent  
    feature dominance.

### Part iii
Construct the confusion matrix from your prediction with a threshold of 1/2 on 
both training and testing data. Explain your accuracy, recall, precision, and 
F1 score to a New Yorker.

```{python}
from sklearn.metrics import confusion_matrix, f1_score, classification_report
# Compute confusion matrices for training and testing data
conf_matrix_train = confusion_matrix(y_train, y_train_pred)
conf_matrix_test = confusion_matrix(y_test, y_test_pred)

# Compute performance metrics for testing data
f1 = f1_score(y_test, y_test_pred, zero_division=0)

# Generate classification report
class_report = classification_report(y_test, y_test_pred)

# results
print("Confusion Matrix - Training Data:")
print(conf_matrix_train)
print("\nConfusion Matrix - Testing Data:")
print(conf_matrix_test)
print("\nF1 Score:", f1)
print("\nClassification Report:")
print(class_report)


```

Confusion Matrices

**Training Data** 
|                    | Predicted Non-Severe | Predicted Severe |
|--------------------|---------------------|------------------|
| **Actual Non-Severe** | 5920                | 8                |
| **Actual Severe**     | 1598                | 8                |

**Testing Data** 
|                    | Predicted Non-Severe | Predicted Severe |
|--------------------|---------------------|------------------|
| **Actual Non-Severe** | 1484                | 2                |
| **Actual Severe**     | 397                 | 1                |


While the model has 79% accuracy, it mostly predicts quick resolutions correctly
(over3d = 0) but fails to identify actual delays.  

Precision, at 33%, means that two-thirds of flagged delays are actually  
resolved quickly, making the model unreliable for predicting extended wait  
times. Recall is 0% for over3d = 1, meaning the model never correctly  
identifies real delays.

With an F1 score of 0.0049, the model fails both ways,it doesn’t detect delays  
and is usually wrong when it does. Despite its high accuracy, it cannot  
predict real delays, making it ineffective. To improve, it needs a better  
algorithm, class balancing, and stronger predictors.  

If you are a New Yorker and your complaint takes 5+ days, this model will almost
NEVER warn you ahead of time. The city might see 79% accuracy and assume the 
model is working, but in reality, it fails at predicting real delays.


### Part iv